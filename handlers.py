"""
í…”ë ˆê·¸ë¨ ë´‡ í•¸ë“¤ëŸ¬ í•¨ìˆ˜ë“¤
"""

import os
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain_groq import ChatGroq
from pinecone import Pinecone, ServerlessSpec
from telegram import ReplyKeyboardRemove, Update
from telegram.ext import ContextTypes, ConversationHandler

from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from bot_config import (
    logger,
    reply_markup_models,
    reply_markup_commands,
    CHOOSING,
    TYPING_REPLY,
    TYPING_CHOICE,
    SUPPORTED_MODELS,
)

PINECONE_INDEX_NAME = "telegram-camera-bot-index"


async def start_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """'/start' ëª…ë ¹ì–´ ì²˜ë¦¬"""
    if not update.message or not update.effective_user:
        logger.warning("ì—…ë°ì´íŠ¸ì— ë©”ì‹œì§€ë‚˜ ì‚¬ìš©ì ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    user = update.effective_user
    welcome_message = (
        f"ì•ˆë…•í•˜ì„¸ìš” {user.name}ë‹˜! ğŸ‘‹\n\n"
        "ì¹´ë©”ë¼ ë§¤ë‰´ì–¼ ë´‡ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!\n"
        "/help ëª…ë ¹ì–´ë¡œ ì‚¬ìš©ë²•ì„ í™•ì¸í•˜ì„¸ìš”."
    )

    await update.message.reply_text(welcome_message, reply_markup=reply_markup_models)


async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """'/help' ëª…ë ¹ì–´ ì²˜ë¦¬"""
    if not update.message:
        logger.warning("ì—…ë°ì´íŠ¸ì— ë©”ì‹œì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    help_text = (
        "ğŸ“¸ <b>ì¹´ë©”ë¼ ë§¤ë‰´ì–¼ ë´‡ ì‚¬ìš©ë²•</b>\n\n"
        "ğŸ”¹ /start - ë´‡ ì‹œì‘\n"
        "ğŸ”¹ /help - ë„ì›€ë§ ë³´ê¸°\n"
        "ğŸ”¹ /manual - ì„¤ëª…ì„œ ê²€ìƒ‰\n\n"
        "/manual ëª…ë ¹ì–´ë¡œ ì‹œì‘í•´ ì£¼ì„¸ìš”!"
    )
    await update.message.reply_html(help_text)


async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """ì¼ë°˜ ë©”ì‹œì§€ ì²˜ë¦¬"""
    if not update.message or not update.effective_user:
        logger.warning("ì—…ë°ì´íŠ¸ì— ë©”ì‹œì§€ë‚˜ ì‚¬ìš©ì ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    user_message = update.message.text
    user_name = update.effective_user.first_name

    logger.info(f"ë©”ì‹œì§€ ìˆ˜ì‹  - ì‚¬ìš©ì: {user_name}, ë‚´ìš©: {user_message}")

    response = (
        f"ì•ˆë…•í•˜ì„¸ìš” {user_name}ë‹˜! ğŸ“¸\n\n"
        f"'{user_message}'ì— ëŒ€í•œ ë‹µë³€ì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤.\n"
        "í˜„ì¬ëŠ” ê¸°ë³¸ ì‘ë‹µë§Œ ì œê³µë©ë‹ˆë‹¤."
    )

    await update.message.reply_text(response)


async def start_manual_conversation(
    update: Update, context: ContextTypes.DEFAULT_TYPE
) -> int:
    """ë§¤ë‰´ì–¼ ê²€ìƒ‰ ëŒ€í™” ì‹œì‘"""
    if not update.message or not update.effective_user:
        logger.warning("ì—…ë°ì´íŠ¸ì— ë©”ì‹œì§€ë‚˜ ì‚¬ìš©ì ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return ConversationHandler.END

    await update.message.reply_text(
        "ë‹¤ìŒ ì¹´ë©”ë¼ ëª¨ë¸ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”:\n",
        reply_markup=reply_markup_models,
    )

    return CHOOSING


async def camera_model_choice(
    update: Update, context: ContextTypes.DEFAULT_TYPE
) -> int:
    """ì¹´ë©”ë¼ ëª¨ë¸ ì„ íƒ ì²˜ë¦¬"""
    if not update.message or context.user_data is None:
        logger.warning("ë©”ì‹œì§€ë‚˜ ì‚¬ìš©ì ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return TYPING_CHOICE

    model = update.message.text

    # ì§€ì›í•˜ëŠ” ëª¨ë¸ì¸ì§€ í™•ì¸
    if model not in SUPPORTED_MODELS:
        await update.message.reply_text(
            f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì…ë‹ˆë‹¤. {', '.join(SUPPORTED_MODELS)} ì¤‘ì—ì„œ ì„ íƒí•´ì£¼ì„¸ìš”.",
            reply_markup=reply_markup_models,
        )
        return CHOOSING

    context.user_data.update({"choice": model})

    await update.message.reply_text(f"{model}ì˜ ì–´ë–¤ ì ì— ëŒ€í•´ ì•Œê³  ì‹¶ìœ¼ì‹ ê°€ìš”?")

    return TYPING_REPLY


async def handle_fallback(update: Update, context: ContextTypes.DEFAULT_TYPE) -> int:
    """ì˜ëª»ëœ ì…ë ¥ ì²˜ë¦¬"""
    if not update.message or not update.effective_user:
        logger.warning("ì—…ë°ì´íŠ¸ì— ë©”ì‹œì§€ë‚˜ ì‚¬ìš©ì ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return TYPING_CHOICE

    await update.message.reply_text(
        f"{', '.join(SUPPORTED_MODELS)} ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”.\n",
        reply_markup=reply_markup_models,
    )

    return CHOOSING


async def query_manual(update: Update, context: ContextTypes.DEFAULT_TYPE) -> int:
    """ë§¤ë‰´ì–¼ ì§ˆë¬¸ ì²˜ë¦¬"""
    if not update.message or not context.user_data:
        logger.warning("ì—…ë°ì´íŠ¸ì— ë©”ì‹œì§€ë‚˜ ì‚¬ìš©ì ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return TYPING_CHOICE

    query = update.message.text
    model = context.user_data.get("choice", "Unknown")

    if not query or not model:
        await update.message.reply_text(
            "ì§ˆë¬¸ì´ ë¹„ì–´ìˆê±°ë‚˜ ëª¨ë¸ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            reply_markup=reply_markup_models,
        )
        return TYPING_CHOICE

    await update.message.reply_html(
        "ğŸ” <b>ê²€ìƒ‰ ì¤‘...</b>\n\n",
        reply_markup=reply_markup_models,
    )

    pinecone_db = None

    embeddings_model = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")
    try:
        pinecone_api_key = os.environ.get("PINECONE_API_KEY")
        pc = Pinecone(api_key=pinecone_api_key)

        if not pc.has_index(PINECONE_INDEX_NAME):
            pc.create_index(
                name=PINECONE_INDEX_NAME,
                dimension=1024,
                metric="cosine",
                spec=ServerlessSpec(cloud="aws", region="us-east-1"),
            )

        index = pc.Index(PINECONE_INDEX_NAME)
        pinecone_db = PineconeVectorStore(index=index, embedding=embeddings_model)

        logger.info(f"ë¡œì»¬ PINECONE DB ë¡œë“œ ì™„ë£Œ: {PINECONE_INDEX_NAME}")
    except Exception as e:
        logger.error(f"PINECONE DB ë¡œë“œ ì‹¤íŒ¨: {e}")
        await update.message.reply_html(
            "ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            reply_markup=reply_markup_models,
        )

    if not pinecone_db:
        await update.message.reply_html(
            "Database ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            reply_markup=reply_markup_models,
        )
        return TYPING_CHOICE

    retriever = pinecone_db.as_retriever(
        search_kwargs={"k": 1, "filter": {"model": model}}
    )

    llm = ChatGroq(
        model="gemma2-9b-it",
        temperature=0.2,
        max_tokens=None,
        timeout=None,
        max_retries=2,
    )

    docs = retriever.invoke(query)

    formatted_docs = []

    for i, (doc) in enumerate(docs):
        metadata = doc.metadata if hasattr(doc, "metadata") else {}

        source_info = []

        if "model" in metadata:
            source_info.append(f"ëª¨ë¸: {metadata['model']}")

        if "page_no" in metadata:
            source_info.append(f"í˜ì´ì§€: {metadata['page_no']}")

        if len(source_info) > 0:
            formatted_docs.append(
                f"ğŸ“š ì°¸ê³  í˜ì´ì§€ {metadata['page_no']}\n"
                f"â€¢ {' | '.join(source_info) if source_info else 'ì¶œì²˜ ì •ë³´ ì—†ìŒ'}\n"
                # f"â€¢ ë‚´ìš©: {doc.page_content[:100]}{'...' if len(doc.page_content) > 100 else ''}"
            )

    result = ""
    for i, doc in enumerate(docs):
        result += "<code>\n"
        result += f">> {doc.page_content}\n\n"
        result += f"{formatted_docs[i]}"
        result += "</code>"

    prompt = PromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ì¹´ë©”ë¼ ë§¤ë‰´ì–¼ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
        ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
        
        ì»¨í…ìŠ¤íŠ¸: {context}
        
        ì§ˆë¬¸: {question}
        
        ë‹µë³€:"""
    )

    chain = (
        {"context": lambda _: result, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    answer = chain.invoke(query)

    help_text = (
        f"ğŸ” {model}: {query}\n\n"
        "ğŸ”¹ <b>ê²€ìƒ‰ ê²°ê³¼</b>:\n"
        f"{answer}\n\n"
        "ğŸ”¹ ë” ê¶ê¸ˆí•œ ê²Œ ìˆìœ¼ì‹ ê°€ìš”?\n"
        "ğŸ”¹ 'DONE'ì„ ì„ íƒí•´ì„œ ëŒ€í™”ë¥¼ ì¢…ë£Œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
    )

    await update.message.reply_html(
        help_text,
        reply_markup=reply_markup_commands,
    )

    return TYPING_REPLY


async def done(update: Update, context: ContextTypes.DEFAULT_TYPE) -> int:
    """ëŒ€í™” ì¢…ë£Œ"""
    if not update.message or not context.user_data:
        logger.warning("ì—…ë°ì´íŠ¸ì— ë©”ì‹œì§€ë‚˜ ì‚¬ìš©ì ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return ConversationHandler.END

    user_data = context.user_data
    if "choice" in user_data:
        del user_data["choice"]

    await update.message.reply_text(
        "ğŸ¤– ëŒ€í™”ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!",
        reply_markup=ReplyKeyboardRemove(),
    )

    user_data.clear()
    return ConversationHandler.END


def facts_to_str(user_data: dict[str, str]) -> str:
    """ì‚¬ìš©ì ë°ì´í„° í¬ë§·íŒ… í—¬í¼ í•¨ìˆ˜"""
    facts = [f"{key} - {value}" for key, value in user_data.items()]
    return "\n".join(facts).join(["\n", "\n"])
